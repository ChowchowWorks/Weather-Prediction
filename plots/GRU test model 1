## This is done on collab so most of the syntex will be catered to collab ##
from google.colab import files

uploaded = files.upload()  # This will prompt you to upload the CSV file

!pip install numpy pandas matplotlib scikit-learn

import pandas as pd

## Load the dataset
file_path = "/content/weather_data (1).csv"
df = pd.read_csv(file_path)

## Removing Outliers

# Check if any relative humidity values are outside [0,1]
outliers = df[(df["relative_humidity"] < 0) | (df["relative_humidity"] > 1)]

# Display any outliers found
print(outliers)
print(f"Total outliers found: {len(outliers)}")

# Remove invalid relative humidity values
df = df[(df["relative_humidity"] >= 0) & (df["relative_humidity"] <= 1)]

# Confirm the cleaning worked
print(f"After cleaning, dataset size: {df.shape}")

outliers = df[(df["relative_humidity"] < 0) | (df["relative_humidity"] > 1)]

# Display any outliers found
print(outliers)
print(f"Total outliers found: {len(outliers)}")

## 80-20 Test split
import numpy as np

# Define the number of time points per group
group_size = 100

# Create a column to track group indices
df = df.copy()  # Ensure we work with a copy to avoid warnings
df.loc[:, 'group'] = df.index // group_size


# Get unique groups and shuffle them
unique_groups = df['group'].unique()
np.random.seed(42)  # Ensure reproducibility
np.random.shuffle(unique_groups)

# Assign 80% of groups to training, 20% to testing
train_groups = unique_groups[:int(0.8 * len(unique_groups))]
test_groups = unique_groups[int(0.8 * len(unique_groups)):]

# Split the dataset
train_df = df[df['group'].isin(train_groups)].drop(columns=['group'])
test_df = df[df['group'].isin(test_groups)].drop(columns=['group'])

# Print the split sizes
print(f"Training set size: {train_df.shape}")
print(f"Testing set size: {test_df.shape}")

## Normalizing the data
from sklearn.preprocessing import MinMaxScaler

# Initialize scaler
scaler = MinMaxScaler()

# Fit on training data and transform both train & test sets
train_scaled = scaler.fit_transform(train_df)
test_scaled = scaler.transform(test_df)

# Convert back to DataFrame
train_df = pd.DataFrame(train_scaled, columns=train_df.columns, index=train_df.index)
test_df = pd.DataFrame(test_scaled, columns=test_df.columns, index=test_df.index)

## Input selection
def create_sequences(data, n_past, n_future):
    X, y = [], []

    for i in range(n_past, len(data) - n_future):
        # Select only the feature columns (first 6 columns)
        X.append(data[i - n_past:i, :-1])  # Exclude the last column (target)

        # Select the target column (last column)
        y.append(data[i + n_future, -1])  # The target variable is the last column

    return np.array(X), np.array(y)

# Define sequence lengths
N_PAST = 24  # Can be tuned
N_FUTURE_OPTIONS = [1, 6, 24]  # Next hour, 6 hours ahead, 1 day ahead

# Generate sequences for each forecasting task
train_sequences = {n_future: create_sequences(train_df.values, N_PAST, n_future) for n_future in N_FUTURE_OPTIONS}
test_sequences = {n_future: create_sequences(test_df.values, N_PAST, n_future) for n_future in N_FUTURE_OPTIONS}

# Print sequence shapes
for key, (X_train, y_train) in train_sequences.items():
    print(f"Forecasting {key} hours ahead: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}")

## GRU Model

import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Define the hyperparameters for the GRU model
GRU_UNITS = 128               # Number of GRU units (neurons) in each GRU layer
DROPOUT_RATE = 0.2            # Dropout rate (20%) to avoid overfitting
EPOCHS = 50                   # Increase number of epochs to 50
BATCH_SIZE = 64               # Size of the batch for training
LEARNING_RATE = 0.001         # Learning rate for Adam optimizer

# Define the function to create and train the model
def create_and_train_test_model(X_train, y_train, X_test, y_test):
    # Initialize the model
    test_model = Sequential()

    # Add the first GRU layer
    test_model.add(GRU(units=GRU_UNITS,              # Number of GRU units
                        activation='tanh',           # Activation function
                        input_shape=(X_train.shape[1], X_train.shape[2]),  # Input shape (sequence_length, num_features)
                        return_sequences=True))      # Return sequences for the next GRU layer

    # Add the second GRU layer
    test_model.add(GRU(units=GRU_UNITS,              # Number of GRU units
                        activation='tanh',           # Activation function
                        return_sequences=False))     # Output single value for the Dense layer

    # Add dropout for regularization
    test_model.add(Dropout(DROPOUT_RATE))  # Dropout rate (20%)

    # Add a Dense output layer
    test_model.add(Dense(1))  # Output a single value (target variable)

    # Compile the model
    test_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),  # Optimizer with learning rate
                       loss='mean_squared_error',  # Loss function for regression
                       metrics=['mae', 'mse'])  # Mean absolute error and mean squared error as metrics

    # Model summary
    test_model.summary()

    # Train the model
    history = test_model.fit(X_train, y_train,
                             epochs=EPOCHS,
                             batch_size=BATCH_SIZE,
                             validation_data=(X_test, y_test))

    # Evaluate the model performance on the test set
    test_loss, test_mae, test_mse = test_model.evaluate(X_test, y_test)

    # Print the evaluation results for the test model
    print(f"Test loss: {test_loss}")
    print(f"Test MAE: {test_mae}")
    print(f"Test MSE: {test_mse}")

    # Plotting training & validation loss and MAE
    plt.figure(figsize=(12, 6))

    # Plot Loss
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()

    # Plot MAE
    plt.subplot(1, 2, 2)
    plt.plot(history.history['mae'], label='Training MAE')
    plt.plot(history.history['val_mae'], label='Validation MAE')
    plt.title('Training & Validation MAE')
    plt.xlabel('Epochs')
    plt.ylabel('Mean Absolute Error (MAE)')
    plt.legend()

    # Show the plots
    plt.tight_layout()
    plt.show()

# Define sequence lengths
N_PAST = 24  # Can be tuned
N_FUTURE_OPTIONS = [1, 6, 24]  # Next hour, 6 hours ahead, 1 day ahead

# Generate sequences for each forecasting task
train_sequences = {n_future: create_sequences(train_df.values, N_PAST, n_future) for n_future in N_FUTURE_OPTIONS}
test_sequences = {n_future: create_sequences(test_df.values, N_PAST, n_future) for n_future in N_FUTURE_OPTIONS}

# Print sequence shapes
for key, (X_train, y_train) in train_sequences.items():
    print(f"Forecasting {key} hours ahead: X_train shape = {X_train.shape}, y_train shape = {y_train.shape}")

# Run the model for each of the forecasting options
for n_future in N_FUTURE_OPTIONS:
    print(f"\nTraining for {n_future} hours ahead forecast:")
    X_train, y_train = train_sequences[n_future]
    X_test, y_test = test_sequences[n_future]

    # Train and evaluate the test model
    create_and_train_test_model(X_train, y_train, X_test, y_test)
